{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "https://github.com/thunlp/MultiRD/tree/master/EnglishReverseDictionary/data\n",
    "\n",
    "- Wordnet\n",
    "- The American Heritage Dictionary\n",
    "- The Collaborative International Dictionary of English\n",
    "- Wiktionary\n",
    "- Websterâ€™s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5d = defaultdict(set)\n",
    "examples_5d = defaultdict(set)\n",
    "\n",
    "# wantwords\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    with open(f'data/data_5d_{split}.json', 'r') as f:\n",
    "        for i in json.load(f):\n",
    "            word = i['word']\n",
    "            defi = i['definitions']\n",
    "            data_5d = add_new_data(data_5d, word, defi)\n",
    "\n",
    "# wordnet (additional 54558 words or phrases)\n",
    "for syn in wn.all_synsets():\n",
    "    word = syn.name().split('.')[0]\n",
    "    defi = syn.definition()\n",
    "    data_5d = add_new_data(data_5d, word, defi)\n",
    "    for example in syn.examples():\n",
    "        examples_5d = add_new_data(examples_5d, word, example)\n",
    "\n",
    "save_data_to_json('data/data_5d.json', data_5d, examples_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5d = load_data_from_json('data/data_5d.json')\n",
    "show_data_stats(data_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5d_with_examples = load_data_from_json('data/data_5d.json', use_examples=True)\n",
    "show_data_stats(data_5d_with_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Supplement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Oxford English Dictionary (https://developer.oxforddictionaries.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oed = load_data_from_json('data/data_oed.json')\n",
    "show_data_stats(data_oed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oed_with_examples = load_data_from_json('data/data_oed.json', use_examples=True)\n",
    "show_data_stats(data_oed_with_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wikipedia (https://huggingface.co/datasets/wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = load_dataset('wikipedia', '20200501.en', split='train')\n",
    "data_wiki = defaultdict(set)\n",
    "\n",
    "for i in tqdm(wikipedia):\n",
    "    word = process_word_name(i['title'])\n",
    "    # only use the first paragraph\n",
    "    defi = i['text'].split('\\n')[0]\n",
    "    data_wiki = add_new_data(data_wiki, word, defi)\n",
    "\n",
    "# remove words that are not in dictionary\n",
    "for word in set(data_wiki.keys()).difference(data_5d.keys()):\n",
    "    data_wiki.pop(word)\n",
    "  \n",
    "save_data_to_json('data/data_wiki.json', data_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of words: 51664\n",
      "num of docs: 56065\n",
      "max doc length: 359\n",
      "min doc length: 1\n",
      "mean doc length: 45.67\n",
      "median doc length: 38.0\n"
     ]
    }
   ],
   "source": [
    "data_wiki = load_data_from_json('data/data_wiki.json')\n",
    "show_data_stats(data_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = defaultdict(set)\n",
    "\n",
    "for data in [data_5d, data_oed, data_wiki]:\n",
    "    for word, defi_set in data.items():\n",
    "        for defi in defi_set:\n",
    "            data_merged[word].add(defi)\n",
    "\n",
    "save_data_to_json('data/data.json', data_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of words: 103874\n",
      "num of docs: 1007305\n",
      "max doc length: 359\n",
      "min doc length: 1\n",
      "mean doc length: 12.88\n",
      "median doc length: 9.0\n"
     ]
    }
   ],
   "source": [
    "data_merged = load_data_from_json('data/data.json')\n",
    "show_data_stats(data_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_with_examples = defaultdict(set)\n",
    "\n",
    "for data in [data_5d_with_examples, data_oed_with_examples, data_wiki]:\n",
    "    for word, defi_set in data.items():\n",
    "        for defi in defi_set:\n",
    "            data_merged_with_examples[word].add(defi)\n",
    "\n",
    "save_data_to_json('data/data_with_examples.json', data_merged_with_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_with_examples = load_data_from_json('data/data_with_examples.json')\n",
    "show_data_stats(data_merged_with_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, out_name):\n",
    "    # back translation\n",
    "    for word in tqdm(words_to_aug(data, 60000)):\n",
    "        for defi in data[word].copy():\n",
    "            trans_defi = back_translate(defi)\n",
    "            if trans_defi:\n",
    "                data[word].add(trans_defi)\n",
    "\n",
    "    save_data_to_json(f'data/{out_name}_trans.json', data)    \n",
    "\n",
    "    # synonym augmentation\n",
    "    synonym_aug = naw.SynonymAug(stopwords=stopwords.words('english'))\n",
    "    for word in tqdm(words_to_aug(data)):\n",
    "        for defi in data[word].copy():\n",
    "            for augmented_text in synonym_aug.augment(defi, n=10):\n",
    "                data[word].add(augmented_text)\n",
    "    \n",
    "    save_data_to_json(f'data/{out_name}_trans_synonym.json', data)    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(data_merged, 'data_augmented')\n",
    "show_data_stats(load_data_from_json('data/data_augmented_trans.json'))\n",
    "show_data_stats(load_data_from_json('data/data_augmented_trans_synonym.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of words: 103874\n",
      "num of docs: 1239899\n",
      "max doc length: 359\n",
      "min doc length: 1\n",
      "mean doc length: 11.96\n",
      "median doc length: 8.0\n"
     ]
    }
   ],
   "source": [
    "# augment_data(data_merged_with_examples, 'data_augmented_with_examples')\n",
    "show_data_stats(load_data_from_json('data/data_augmented_with_examples_trans.json'))\n",
    "show_data_stats(load_data_from_json('data/data_augmented_with_examples_trans_synonym.json'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df826989e34d77a4e9898cce247dd42851711639ab0a5445b4411476078816a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
